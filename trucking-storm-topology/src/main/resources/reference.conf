# For a list of constants for all the configurations possible on a Storm cluster and Storm topology, check out:
# http://storm.apache.org/releases/current/javadocs/org/apache/storm/Config.html
#
# Default values are available and can be seen here:
# https://github.com/apache/storm/blob/v1.0.2/conf/defaults.yaml

trucking-storm-topology {

  topology {

    # Specifies how many processes you want allocated around the cluster to execute the topology.
    # Each component in the topology will execute as many threads. The number of threads allocated to a given
    # component is configured through the setBolt and setSpout methods. Those threads exist within worker processes.
    # Each worker process contains within it some number of threads for some number of components.
    # For instance, you may have 300 threads specified across all your components and 50 worker processes specified
    # in your config. Each worker process will execute 6 threads, each of which of could belong to a different component.
    # You tune the performance of Storm topologies by tweaking the parallelism for each component and the number of
    # worker processes those threads should run within.
    #
    # Config path can be fetched using the constant org.apache.storm.Config.TOPOLOGY_WORKERS
    workers = 1

    # The maximum amount of time given to the topology to fully process a message
    # emitted by a spout. If the message is not acked within this time frame, Storm
    # will fail the message on the spout. Some spouts implementations will then replay
    # the message at a later time.
    #
    # Config path can be fetched using the constant org.apache.storm.Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS
    message.timeout.secs = 120

    # When set to true, tells Storm to log every message every emitted by a component.
    # This is useful in local mode when testing topologies, but you probably want to keep this turned off
    # when running topologies on the cluster.
    #
    # Config path can be fetched using the constant org.apache.storm.Config.TOPOLOGY_DEBUG
    debug = false

    # How many instances to create for a spout/bolt. A task runs on a thread with zero or more
    # other tasks for the same spout/bolt. The number of tasks for a spout/bolt is always
    # the same throughout the lifetime of a topology, but the number of executors (threads) for
    # a spout/bolt can change over time. This allows a topology to scale to more or less resources
    # without redeploying the topology or violating the constraints of Storm (such as a fields grouping
    # guaranteeing that the same value goes to the same task).
    #
    # Config path can be fetched using the constant org.apache.storm.Config.TOPOLOGY_TASKS
    tasks = 1

    # Bolt-specific configuration for windowed bolts to specify the window length in time duration.
    #
    # Config path can be fetched using the constant org.apache.storm.Config.TOPOLOGY_BOLTS_WINDOW_LENGTH_DURATION_MS
    bolts.window.length.duration.ms = 20000

    # Bolt-specific configuration for windowed bolts to specify the sliding interval as a count of number of tuples.
    #
    # Config path can be fetched using the constant org.apache.storm.Config.TOPOLOGY_BOLTS_SLIDING_INTERVAL_COUNT
    bolts.window.sliding.interval.count = 10


    # The maximum parallelism allowed for a component in this topology. This configuration is
    # typically used in testing to limit the number of threads spawned in local mode.
    #
    # Config path can be fetched using the constant org.apache.storm.Config.TOPOLOGY_MAX_TASK_PARALLELISM
    max.task.parallelism = 1
  }

  nimbus.seeds = ["nimbus.storm-app-1.root.hwx.site"]
  jar.path = "/Users/eorendain/Documents/trucking/trucking-iot/trucking-storm-topology/target/scala-2.11/trucking-storm-topology-assembly-0.4.0-SNAPSHOT.jar"

  storm {

    # A list of hosts of ZooKeeper servers used to manage the cluster.
    #
    # Config path can be fetched using the constant org.apache.storm.Config.STORM_ZOOKEEPER_SERVERS
    #zookeeper.servers = "localhost"

    # While meant for Storm ZK, used in code explicitely for Kafka ZK.
    zookeeper.servers = "yprod001.l42scl.hortonworks.com"

    # The root location at which Storm stores data in ZooKeeper.
    #
    # Config path can be fetched using the constant org.apache.storm.Config.STORM_ZOOKEEPER_ROOT
    #zookeeper.root = "/storm"

    # Again, used in code explicitely for Kafka
    #zookeeper.root = "/services/slider/users/root/kafka-app-2"
    zookeeper.root = "/storm-storm-app-1"

  }

  nifi {

    # NiFi URL
    #
    # Config path can be fetched using the constant TruckingTopology.NiFiUrl
    url = "http://sandbox-hdf.hortonworks.com:9090/nifi"

    # From NiFi:
    #
    # port-name: NiFi output port name
    # batch-size: Requested NiFI batch count

    truck-data {
      port-name = "Enriched Truck Data"
      batch-size = 10
    }

    traffic-data {
      port-name = "Traffic Data"
      batch-size = 10
    }

    # To NiFi:
    #
    # port-name: NiFi output port name
    # batch-size: NiFI batch size
    # tick-frequency: Tick frequency, in seconds, to push to NiFi

    truck-and-traffic-data {
      port-name = "Enriched Truck And Traffic Data"
      batch-size = 1
      tick-frequency = 1
    }

    driver-stats {
      port-name = "Windowed Driver Stats"
      batch-size = 1
      tick-frequency = 1
    }
  }

  kafka {

    # A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.
    # The client will make use of all servers irrespective of which servers are specified here for bootstrapping.
    # This list only impacts the initial hosts used to discover the full set of servers.
    bootstrap-servers = ["sandbox.hortonworks.com:6667", "sandbox.hortonworks.com:6667"]
    #bootstrap-servers = ["kafkabroker1.kafka-app-2.root.hwx.site:9092"]

    # Serializer class for key that implements the Serializer interface.
    key-serializer = "org.apache.kafka.common.serialization.StringSerializer"

    # Serializer class for value that implements the Serializer interface.
    value-serializer = "org.apache.kafka.common.serialization.StringSerializer"

    # Avro serializer class for value that implements the Serializer interface.
    avro-value-serializer = "com.hortonworks.registries.schemaregistry.serdes.avro.kafka.KafkaAvroSerializer"

    # A unique string that identifies the consumer group this consumer belongs to.
    # This property is required if the consumer uses either the group management functionality by
    # using subscribe(topic) or the Kafka-based offset management strategy.
    #
    # More info at: http://kafka.apache.org/documentation.html#newconsumerconfigs
    group-id = "d"

    # Name of the Kafka topic for truck data.
    truck-data.topic = "trucking_data_truck"

    # Name of the Kafka topic for traffic data.
    traffic-data.topic = "trucking_data_traffic"

    # Name of the Kafka topic for fully joined trucking data.
    joined-data.topic = "trucking_data_joined"

    # Name of the Kafka topic for windowed driver stats.
    driver-stats.topic = "trucking_data_driverstats"
  }

  hbase {

    #event-key-field = "eventKey"

    # The HBase column family shared by all relevant tables.
    column-family = "c"

    # Name of the HBase table storing the fully joined trucking data.
    trucking-data.table = "trucking_data_joined"
  }

  schema-registry {
    url = "http://sandbox-hdf.hortonworks.com:15005/api/v1"
  }
}
